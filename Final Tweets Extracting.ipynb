{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fd8e15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import jsonlines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from pandas import Series\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import f_oneway\n",
    "from datetime import timedelta\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "csv_file = r'E:\\Documentos\\Jorge\\University\\Master\\Semester 1\\ECMM443 Introduction to Data Science\\Tweets\\tweets.csv'\n",
    "working_directory = r'D:\\Documentos\\Jorge\\University\\Master\\Semester 1\\Tweets\\geoEurope'\n",
    "\n",
    "\n",
    "\n",
    "tweets_missed = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc8b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_missed = []\n",
    "header = ['tweet_id', 'timestamp', 'user_id', 'user_name', 'full_text', 'mentions_users_ids', 'mentions_users_names', 'country_code', 'coordinate_lon', 'coordinate_lat', 'point1_lon', 'point1_lat', 'point2_lon', 'point2_lat', 'point3_lon', 'point3_lat', 'point4_lon', 'point4_lat']\n",
    "\n",
    "start = time.time()\n",
    "with open(csv_file, 'w', encoding='UTF8', newline='') as c:\n",
    "    x = 1\n",
    "    y = 1\n",
    "    writer = csv.DictWriter(c, fieldnames=header)\n",
    "    c.truncate()\n",
    "    writer.writeheader()\n",
    "    for file_name in [file for file in os.listdir(working_directory) if file.endswith('.json')]:\n",
    "        with jsonlines.open(working_directory + '\\\\' + file_name, 'r') as f:\n",
    "            print('Started: ' + str(x) + ' ' + file_name)\n",
    "            x+=1\n",
    "            for each in f:\n",
    "                y = y + 1\n",
    "                tweet = {}\n",
    "                mentions_users_ids = []\n",
    "                mentions_users_names = []\n",
    "                if 'created_at' in each:\n",
    "                    tweet['tweet_id'] = each['id']\n",
    "                    tweet['timestamp'] = int(each['timestamp_ms'])\n",
    "                    tweet['user_id'] = each['user']['id']\n",
    "                    tweet['user_name'] = each['user']['screen_name']\n",
    "                    if 'extended_tweet' in each:\n",
    "                        if 'full_text' in each['extended_tweet']:\n",
    "                            if each['extended_tweet']['full_text'] is not None: \n",
    "                                tweet['full_text'] = repr(each['extended_tweet']['full_text'])\n",
    "                        if 'entities' in each['extended_tweet']:\n",
    "                            if 'user_mentions' in each['extended_tweet']['entities']:\n",
    "                                if each['extended_tweet']['entities']['user_mentions']:\n",
    "                                    if len(each['extended_tweet']['entities']['user_mentions']) > 1:\n",
    "                                        current_ids = ''\n",
    "                                        current_names = ''\n",
    "                                        for i in range(0, len(each['extended_tweet']['entities']['user_mentions'])):\n",
    "                                            if i == 0:\n",
    "                                                mentions_ids = str(each['extended_tweet']['entities']['user_mentions'][i]['id'])\n",
    "                                                current_ids = mentions_ids\n",
    "                                                mentions_names = str(each['extended_tweet']['entities']['user_mentions'][i]['screen_name'])\n",
    "                                                current_names = mentions_names\n",
    "                                                continue\n",
    "                                            mentions_ids = str(each['extended_tweet']['entities']['user_mentions'][i]['id'])\n",
    "                                            current_ids = current_ids + ', ' + mentions_ids\n",
    "                                            mentions_names = str(each['extended_tweet']['entities']['user_mentions'][i]['screen_name'])\n",
    "                                            current_names = current_names + ', ' + mentions_names\n",
    "                                        tweet['mentions_users_ids'] = current_ids\n",
    "                                        tweet['mentions_users_names'] = current_names\n",
    "                                    else:\n",
    "                                        tweet['mentions_users_ids'] = each['extended_tweet']['entities']['user_mentions'][0]['id']\n",
    "                                        tweet['mentions_users_names'] = each['extended_tweet']['entities']['user_mentions'][0]['screen_name']\n",
    "                    else:\n",
    "                        if each['entities']['user_mentions']:\n",
    "                            if len(each['entities']['user_mentions']) > 1:\n",
    "                                current_ids = ''\n",
    "                                current_names = ''\n",
    "                                for i in range(0, len(each['entities']['user_mentions'])):\n",
    "                                    if i == 0:\n",
    "                                        mentions_ids = str(each['entities']['user_mentions'][i]['id'])\n",
    "                                        current_ids = mentions_ids\n",
    "                                        mentions_names = str(each['entities']['user_mentions'][i]['screen_name'])\n",
    "                                        current_names = mentions_names\n",
    "                                        continue\n",
    "                                    mentions_ids = str(each['entities']['user_mentions'][i]['id'])\n",
    "                                    current_ids = current_ids + ', ' + mentions_ids\n",
    "                                    mentions_names = str(each['entities']['user_mentions'][i]['screen_name'])\n",
    "                                    current_names = current_names + ', ' + mentions_names\n",
    "                            else:\n",
    "                                tweet['mentions_users_ids'] = each['entities']['user_mentions'][0]['id']\n",
    "                                tweet['mentions_users_names'] = each['entities']['user_mentions'][0]['screen_name']\n",
    "                        tweet['full_text'] = repr(each['text'])\n",
    "                    if each['place']:\n",
    "                        if each['place']['country_code']:\n",
    "                            tweet['country_code'] = str(each['place']['country_code'])\n",
    "                        if each['place']['bounding_box']:\n",
    "                            tweet['point1_lon'] = each['place']['bounding_box']['coordinates'][0][0][0]\n",
    "                            tweet['point1_lat'] = each['place']['bounding_box']['coordinates'][0][0][1]\n",
    "\n",
    "                            tweet['point2_lon'] = each['place']['bounding_box']['coordinates'][0][1][0]\n",
    "                            tweet['point2_lat'] = each['place']['bounding_box']['coordinates'][0][1][1]\n",
    "\n",
    "                            tweet['point3_lon'] = each['place']['bounding_box']['coordinates'][0][2][0]\n",
    "                            tweet['point3_lat'] = each['place']['bounding_box']['coordinates'][0][2][1]\n",
    "\n",
    "                            tweet['point4_lon'] = each['place']['bounding_box']['coordinates'][0][3][0]\n",
    "                            tweet['point4_lat'] = each['place']['bounding_box']['coordinates'][0][3][1]\n",
    "                    if each['coordinates']:\n",
    "                        tweet['coordinate_lon'] = each['coordinates']['coordinates'][0]\n",
    "                        tweet['coordinate_lat'] = each['coordinates']['coordinates'][1]\n",
    "                    writer.writerow(tweet)\n",
    "                elif 'tweets_missed':\n",
    "                    tweets_missed.append(each[\"tweets_missed\"])\n",
    "                else:\n",
    "                    print('other line error:', y)\n",
    "                    continue \n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3b74d2",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d12c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "part1_cols = ['tweet_id', 'timestamp', 'user_id']\n",
    "df = pd.read_csv(csv_file, usecols = part1_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f4ea56",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9552e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total number of tweets\n",
    "num_tweets = df['tweet_id'].count()\n",
    "print('There are a total of', num_tweets, 'tweets present')\n",
    "\n",
    "#Remove duplicates\n",
    "df_unique = df.drop_duplicates()\n",
    "df_unique = df_unique.dropna(how='all', axis=0)\n",
    "print('After removing duplicate tweets, there are a total of', len(df_unique), 'unique tweets present')\n",
    "\n",
    "#Tweets missing\n",
    "print(sum(tweets_missed), 'tweets are missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a9af15",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_users = df['user_id'].drop_duplicates()\n",
    "print('There are', unique_users.count(), 'unique users')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea634d3",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6581c60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_df = df_unique.copy()\n",
    "time_series_df = time_series_df.dropna(how='all', axis=0)\n",
    "time_series_df = time_series_df.reset_index(drop=True)\n",
    "\n",
    "index_time = []\n",
    "for i in range(0, len(time_series_df['timestamp'])):\n",
    "    timestamp = datetime.datetime.fromtimestamp(int(time_series_df['timestamp'][i]) / 1000)\n",
    "    index_time.append(timestamp)\n",
    "    \n",
    "\n",
    "time_series_df.insert(1, 'index_times', index_time)\n",
    "time_series_df = time_series_df.set_index(time_series_df['index_times'])\n",
    "time_series_df = time_series_df.drop('timestamp', axis=1)\n",
    "\n",
    "def f(x):\n",
    "     return Series(dict(Number_of_tweets = x['tweet_id'].count(), ))\n",
    "\n",
    "daily_count = time_series_df.groupby(time_series_df.index.date).apply(f)\n",
    "print('There are', len(daily_count), 'days in the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd740e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.xticks(rotation = 30, fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel('Dates', labelpad=30, fontsize=20)\n",
    "plt.ylabel('Number of tweets', labelpad=30, fontsize=20)\n",
    "plt.title('Number of Tweets per day', fontsize=25, pad=20)\n",
    "plt.plot(daily_count['Number_of_tweets'], lw=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b525bb1",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20688e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_by_day = time_series_df['tweet_id'].groupby(by=time_series_df['index_times'].dt.date).count()\n",
    "\n",
    "weekday = []\n",
    "weekend = []\n",
    "\n",
    "for each in range(0, len(count_by_day)):\n",
    "    if count_by_day.index[each].weekday() < 5:\n",
    "        weekday.append(count_by_day[each])\n",
    "    else:\n",
    "        weekend.append(count_by_day[each])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f501d24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "plt.setp(ax, xticklabels=['Weekday', 'Weekend'])\n",
    "plt.ylabel('Number of Tweets', fontsize=14, labelpad=20)\n",
    "plt.xticks(fontsize=13)plt.yticks(fontsize=13)\n",
    "plt.title('Average number of tweets on weekdays VS weekends', fontsize=20, pad=20)\n",
    "plt.boxplot([weekday, weekend]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf1186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistcally significant\n",
    "print(f_oneway(weekday, weekend))\n",
    "print('There are not enough statisitcally significant differences between the number of tweets on weekdays and weekends')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ac1145",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf264b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_by_day2 = time_series_df['tweet_id'].groupby(by \\\n",
    "                            = [time_series_df['index_times'].dt.date, time_series_df['index_times'].dt.hour]).count()\n",
    "\n",
    "count_hour = []\n",
    "count_index = []\n",
    "\n",
    "for each in range(0, len(count_by_day2)):\n",
    "    if count_by_day2.index[each][0].weekday() < 5:\n",
    "        count_hour.append(count_by_day2[each])\n",
    "        count_index.append(count_by_day2.index[each][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f334e55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_series = pd.Series(count_hour, count_index)\n",
    "count_my_series = my_series.groupby(by = my_series.index).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db06331",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "ax.ticklabel_format(style='plain')\n",
    "plt.xticks(np.arange(0,24,2), fontsize=13);\n",
    "plt.yticks(fontsize=13)\n",
    "plt.title('Number of Tweets per hour on Weekdays', fontsize=25, pad=20)\n",
    "plt.plot(count_my_series.index, count_my_series.values);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b72018",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8784bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "part2_cols = ['tweet_id', 'user_id', 'mentions_users_ids', 'country_code']\n",
    "df = pd.read_csv(csv_file, usecols = part2_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef3f366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.dropna(how='all', axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eb2376",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['mentions_users_ids'], inplace=True)\n",
    "\n",
    "for i in range(0, len(df['mentions_users_ids'].values)):\n",
    "    df['mentions_users_ids'].values[i] = df['mentions_users_ids'].values[i].split(\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8e429b",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9457cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tweets = df['user_id'].groupby(by = df['user_id']).count()\n",
    "count_tweets = count_tweets.groupby(by=count_tweets.values).count()\n",
    "df_count = pd.DataFrame(data = count_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafb6e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "plt.hist(df_count, bins= 25, range = [0, 15000], log = True)\n",
    "plt.xticks(np.arange(0, 15000, 1000))\n",
    "plt.title('Histogram of the number of users and the tweets they make', fontsize=20)\n",
    "plt.xlabel('Number of tweets', labelpad=30, fontsize=20)\n",
    "plt.ylabel('Number of users (log scale)', labelpad=30, fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a3c34c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "plt.hist(df_count, bins= 10000)\n",
    "plt.xlim(0, 250)\n",
    "plt.xticks(np.arange(0, 250, 50))\n",
    "plt.title('Histogram of the number of users and the tweets they make', fontsize=20)\n",
    "plt.xlabel('Number of tweets', labelpad=30, fontsize=20)\n",
    "plt.ylabel('Number of users', labelpad=30, fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46af38d",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710c62bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tweets = df_unique['user_id'].groupby(by = df_unique['user_id']).count()\n",
    "\n",
    "tweeets = np.sort(count_tweets.values)\n",
    "number_tweets = tweeets[-5:]\n",
    "\n",
    "max_users_ids = []\n",
    "for i in range(0, 5):\n",
    "    max_users_ids.append(count_tweets[count_tweets == number_tweets[i]].index[0])\n",
    "\n",
    "df_q2 = df_unique[['user_id','user_name']]\n",
    "df_q2 = df_q2.drop_duplicates(subset='user_name')\n",
    "\n",
    "users_names = []\n",
    "for each in max_users_ids:\n",
    "    users_names.append(df_q2['user_name'][df_q2['user_id'] == each])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042dd001",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54381308",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_ids = df['user_id']\n",
    "tweet_ids = df['tweet_id']\n",
    "\n",
    "df11 = pd.DataFrame({'mentions_users_ids' :mentions_users_ids, 'user_id': user_ids, 'tweet_ids': tweet_ids})\n",
    "df11 = df11.dropna(subset='mentions_users_ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d29416",
   "metadata": {},
   "outputs": [],
   "source": [
    "how_many_users_mentions = []\n",
    "for each in df['mentions_users_ids'].values:\n",
    "    if len(each) > 1:\n",
    "        for i in range(0, len(each)):\n",
    "            how_many_users_mentions.append(each[i])\n",
    "    else:\n",
    "        how_many_users_mentions.append(each[0])\n",
    "        \n",
    "how_many_users_mentions_count = dict(Counter(how_many_users_mentions))\n",
    "print(sorted(how_many_users_mentions_count.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199f58e0",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78fec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['mentions_users_ids', 'country_code'], inplace=True)\n",
    "\n",
    "country_codes = ['GB', 'IE', 'ES', 'FR']\n",
    "df_GB = df.loc[df['country_code'] == 'GB']\n",
    "df_IE = df.loc[df['country_code'] == 'IE']\n",
    "df_FR = df.loc[df['country_code'] == 'FR']\n",
    "df_ES = df.loc[df['country_code'] == 'ES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd1e95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_mentions_list_GB = []\n",
    "for each in df_GB['mentions_users_ids'].values:\n",
    "    if len(each) > 1:\n",
    "        for i in range(0, len(each)):\n",
    "            users_mentions_list_GB.append(int(each[i]))\n",
    "    else:\n",
    "        users_mentions_list_GB.append(int(each[0]))\n",
    "\n",
    "users_mentions_list_IE = []\n",
    "for each in df_IE['mentions_users_ids'].values:\n",
    "    if len(each) > 1:\n",
    "        for i in range(0, len(each)):\n",
    "            users_mentions_list_IE.append(int(each[i]))\n",
    "    else:\n",
    "        users_mentions_list_IE.append(int(each[0]))\n",
    "\n",
    "users_mentions_list_FR = []\n",
    "for each in df_FR['mentions_users_ids'].values:\n",
    "    if len(each) > 1:\n",
    "        for i in range(0, len(each)):\n",
    "            users_mentions_list_FR.append(int(each[i]))\n",
    "    else:\n",
    "        users_mentions_list_FR.append(int(each[0]))\n",
    "\n",
    "users_mentions_list_ES = []\n",
    "for each in df_ES['mentions_users_ids'].values:\n",
    "    if len(each) > 1:\n",
    "        for i in range(0, len(each)):\n",
    "            users_mentions_list_ES.append(int(each[i]))\n",
    "    else:\n",
    "        users_mentions_list_ES.append(int(each[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e0dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['user_id'].isin(users_mentions_list_GB)].loc[df['country_code'].isin(country_codes)].groupby(by='country_code').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22368457",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['user_id'].isin(users_mentions_list_IE)].loc[df['country_code'].isin(country_codes)].groupby(by='country_code').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5f6db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['user_id'].isin(users_mentions_list_FR)].loc[df['country_code'].isin(country_codes)].groupby(by='country_code').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e5ff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['user_id'].isin(users_mentions_list_ES)].loc[df['country_code'].isin(country_codes)].groupby(by='country_code').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9340c80a",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6198eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "part3_cols = ['tweet_id', 'country_code', 'coordinate_lon', 'coordinate_lat']\n",
    "df = pd.read_csv(csv_file, usecols = part3_cols)\n",
    "\n",
    "#Remove duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.dropna(how='all', axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4641bf91",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e23b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset='coordinate_lon', inplace=True)\n",
    "\n",
    "gdf = geopandas.GeoDataFrame(df, geometry=geopandas.points_from_xy(df.coordinate_lon, df.coordinate_lat))\n",
    "gdf.reset_index(drop=True, inplace=True)\n",
    "gdf.set_crs(crs=4326, epsg=4326, inplace=True)\n",
    "\n",
    "world_map = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "world_map = world_map[['name', 'geometry']]\n",
    "world_map.to_crs(crs=4326, epsg=4326, inplace=True)\n",
    "\n",
    "join_left_df = gpd.sjoin(gdf, world_map, how=\"left\").groupby(by = 'name').count().reset_index()\n",
    "\n",
    "final = pd.merge(join_left_df, world_map, on='name')\n",
    "final.drop(['country_code', 'coordinate_lon', 'coordinate_lat', 'geometry_x', 'index_right'], axis = 1, inplace = True)\n",
    "final.rename({'tweet_id':'count', 'geometry_y': 'geometry'}, axis = 1, inplace = True)\n",
    "\n",
    "gdf_final = gpd.GeoDataFrame(final, crs=\"EPSG:4326\", geometry='geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169d6a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(11, 7))\n",
    "gdf_final.plot(ax=ax, column='count', cmap='YlOrRd', legend=False)\n",
    "plt.xlim(-24.5, 69.1)\n",
    "plt.ylim(34.8, 81.9)\n",
    "plt.title('Number of tweets per country in June 2022', fontsize = 20)\n",
    "\n",
    "cax = fig.add_axes([1, 0.1, 0.03, 0.8])\n",
    "vmin = gdf_final['count'].min()\n",
    "vmax = gdf_final['count'].max()\n",
    "sm = plt.cm.ScalarMappable(cmap='YlOrRd', norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "cbr = fig.colorbar(sm, cax=cax)\n",
    "cbr.ax.tick_params(labelsize=20) \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5259b9b",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c790867",
   "metadata": {},
   "outputs": [],
   "source": [
    "part34_cols = ['tweet_id', 'country_code']\n",
    "df = pd.read_csv(csv_file, usecols = part34_cols)\n",
    "\n",
    "#Remove duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.dropna(how='all', axis=0, inplace=True)\n",
    "\n",
    "world_map = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "# world_map = world_map[['name', 'geometry']]\n",
    "world_map.to_crs(crs=4326, epsg=4326, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb331cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'D:\\Documentos\\Jorge\\University\\Master\\Tweets\\Electricity_use_per_person.csv')\n",
    "df = df[['Country', '2011']]\n",
    "\n",
    "df_joined = df.set_index('Country').join(world_map.set_index('name'))\n",
    "df_joined = gpd.GeoDataFrame(df_joined, crs=\"EPSG:4326\", geometry='geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c2b2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "df_joined.plot(ax=ax, column='2011', cmap='YlOrRd', legend=False)\n",
    "plt.xlim(-20, 40)\n",
    "plt.ylim(34, 73)\n",
    "plt.title('Electricity use per capita in 2011 (kW/h)', fontsize = 20)\n",
    "\n",
    "cax = fig.add_axes([1, 0.1, 0.03, 0.8])\n",
    "vmin = df_joined['2011'].min()\n",
    "vmax = df_joined['2011'].max()\n",
    "sm = plt.cm.ScalarMappable(cmap='YlOrRd', norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "cbr = fig.colorbar(sm, cax=cax)\n",
    "cbr.ax.tick_params(labelsize=20) \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08b5078",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ec74a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "part33_cols = ['tweet_id', 'coordinate_lon', 'point1_lon', 'point1_lat', 'point2_lon', 'point2_lat', 'point3_lon', 'point3_lat',]\n",
    "df = pd.read_csv(csv_file, usecols = part33_cols)\n",
    "\n",
    "#Remove duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.dropna(how='all', axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f795d0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df['coordinate_lon'].isnull()]\n",
    "df1 = df1.dropna(subset=['point1_lon'])\n",
    "df1 = df1[['point1_lon', 'point1_lat', 'point3_lon', 'point3_lat']]\n",
    "df1 = df1.reset_index(drop=True)\n",
    "\n",
    "df_point1 = gpd.points_from_xy(df1.point1_lon, df1.point1_lat)\n",
    "df_point3 = gpd.points_from_xy(df1.point3_lon, df1.point3_lat)\n",
    "dist = df_point1.distance(df_point3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d759a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "plt.hist(dist, bins = 100, histtype = 'step', cumulative = True);\n",
    "plt.title('CDF plot of the bounding box diagonals', fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220f97e9",
   "metadata": {},
   "source": [
    "# Part 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c58db77",
   "metadata": {},
   "outputs": [],
   "source": [
    "part4_cols = ['tweet_id', 'timestamp', 'full_text', 'country_code']\n",
    "df = pd.read_csv(csv_file, usecols = part4_cols)\n",
    "\n",
    "#Remove duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.dropna(how='all', axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ec1461",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a874eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_df = df.copy()\n",
    "time_series_df.dropna(how='all', axis=0, inplace=True)\n",
    "time_series_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3374e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_time = []\n",
    "for i in range(0, len(time_series_df['timestamp'])):\n",
    "    timestamp = datetime.date.fromtimestamp(int(time_series_df['timestamp'][i]) / 1000)\n",
    "    index_time.append(timestamp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47baea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_df.insert(1, 'index_times', index_time)\n",
    "time_series_df.drop('timestamp', axis=1, inplace=True)\n",
    "time_series_df.set_index(time_series_df['index_times'], drop=True, inplace = True)\n",
    "time_series_df.drop('index_times', axis=1, inplace=True)\n",
    "time_series_df = time_series_df[~time_series_df['country_code'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291f0cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IE = time_series_df.loc[time_series_df['country_code'] == 'IE']\n",
    "\n",
    "df_FR = time_series_df.loc[time_series_df['country_code'] == 'FR']\n",
    "\n",
    "df_UK = time_series_df.loc[time_series_df['country_code'] == 'GB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a6363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IE_day = df_IE.loc[df_IE.index == datetime.date(2022, 6, 26)]\n",
    "df_FR_day = df_FR.loc[df_FR.index == datetime.date(2022, 6, 19)]\n",
    "df_UK_day = df_UK.loc[df_UK.index == datetime.date(2022, 6, 24)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbdfa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_IE = df_IE_day['full_text'].apply(lambda x: pd.value_counts(re.findall('(#\\w+)', x.lower() )))\\\n",
    "                     .sum(axis=0)\\\n",
    "                     .to_frame()\\\n",
    "                     .reset_index()\\\n",
    "                     .sort_values(by=0,ascending=False)\n",
    "hashtags_IE.columns = ['hashtag','occurences']\n",
    "\n",
    "hashtags_IE[:10].plot(kind='bar',y='occurences',x='hashtag')\n",
    "plt.title('Top 10 Hashtags for Ireland on the 26-06-2022', fontsize=22);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d1fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_FR = df_FR_day['full_text'].apply(lambda x: pd.value_counts(re.findall('(#\\w+)', x.lower() )))\\\n",
    "                     .sum(axis=0)\\\n",
    "                     .to_frame()\\\n",
    "                     .reset_index()\\\n",
    "                     .sort_values(by=0,ascending=False)\n",
    "hashtags_FR.columns = ['hashtag','occurences']\n",
    "\n",
    "hashtags_FR[:10].plot(kind='bar',y='occurences',x='hashtag')\n",
    "plt.suptitle('Top 10 Hashtags for France on the 19-06-2022', fontsize=22);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb7816",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_UK_1 = df_UK_day['full_text'][0:33299].apply(lambda x: pd.value_counts(re.findall('(#\\w+)', x.lower() )))\\\n",
    "                     .sum(axis=0)\\\n",
    "                     .to_frame()\\\n",
    "                     .reset_index()\\\n",
    "                     .sort_values(by=0,ascending=False)\n",
    "hashtags_UK_1.columns = ['hashtag','occurences']\n",
    "\n",
    "hashtags_UK_2 = df_UK_day['full_text'][33299:66597].apply(lambda x: pd.value_counts(re.findall('(#\\w+)', x.lower() )))\\\n",
    "                     .sum(axis=0)\\\n",
    "                     .to_frame()\\\n",
    "                     .reset_index()\\\n",
    "                     .sort_values(by=0,ascending=False)\n",
    "hashtags_UK_2.columns = ['hashtag','occurences']\n",
    "\n",
    "hashtags_UK_3 = df_UK_day['full_text'][66597:99896].apply(lambda x: pd.value_counts(re.findall('(#\\w+)', x.lower() )))\\\n",
    "                     .sum(axis=0)\\\n",
    "                     .to_frame()\\\n",
    "                     .reset_index()\\\n",
    "                     .sort_values(by=0,ascending=False)\n",
    "hashtags_UK_3.columns = ['hashtag','occurences']\n",
    "\n",
    "hashtags_UK_4 = df_UK_day['full_text'][99896:].apply(lambda x: pd.value_counts(re.findall('(#\\w+)', x.lower() )))\\\n",
    "                     .sum(axis=0)\\\n",
    "                     .to_frame()\\\n",
    "                     .reset_index()\\\n",
    "                     .sort_values(by=0,ascending=False)\n",
    "hashtags_UK_4.columns = ['hashtag','occurences']\n",
    "\n",
    "hashtags_UK = pd.concat([hashtags_UK_1, hashtags_UK_2, hashtags_UK_3, hashtags_UK_4]).groupby(['hashtag']).sum().reset_index()\n",
    "\n",
    "hashtags_UK.sort_values(by=['occurences'], ascending=False, inplace=True)\n",
    "\n",
    "hashtags_UK[:10].plot(kind='bar',y='occurences',x='hashtag')\n",
    "plt.suptitle('Top 10 Hashtags for Great Britain on the 24-06-2022', fontsize=22);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65163054",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "additional  = ['rt','rts','retweet']\n",
    "swords_IE = set().union(stopwords.words('english'),additional)\n",
    "swords_FR = set().union(stopwords.words('french'),additional) \n",
    "swords_UK = set().union(stopwords.words('english'),additional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65cce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "swords_FR.add('c\\'est')\n",
    "swords_FR.add('ça')\n",
    "swords_FR.add('c’est')\n",
    "swords_FR.add('j\\'ai')\n",
    "swords_FR.add(r'j\\'ai')\n",
    "swords_FR.add('l\\'')\n",
    "swords_FR.add('d\\'')\n",
    "swords_FR.add('j\\'')\n",
    "swords_FR.add('l’')\n",
    "swords_FR.add('j’')\n",
    "swords_FR.add('d’')\n",
    "swords_FR.add('n’')\n",
    "swords_FR.add(r'j\\'')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53820bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "textIE = []\n",
    "for each in df_IE_day['full_text']:\n",
    "    textIE.append(literal_eval(each))\n",
    "df_IE_use = pd.DataFrame(textIE, columns=['text'])\n",
    "\n",
    "textFR = []\n",
    "for each in df_FR_day['full_text']:\n",
    "    textFR.append(literal_eval(each))\n",
    "df_FR_use = pd.DataFrame(textFR, columns=['text'])\n",
    "\n",
    "textUK = []\n",
    "for each in df_UK_day['full_text']:\n",
    "    textUK.append(literal_eval(each))\n",
    "df_UK_use = pd.DataFrame(textUK, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01da4023",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IE_use['processed_text'] = df_IE_use['text'].str.lower()\\\n",
    "          .str.replace('(@[a-z0-9]+)\\w+',' ')\\\n",
    "          .str.replace('(http\\S+)', ' ')\\\n",
    "          .str.replace('([^0-9a-z \\t])',' ')\\\n",
    "          .str.replace(' +',' ')\\\n",
    "          .str.replace('amp',' ')\\\n",
    "          .apply(lambda x: [i for i in x.split() if not i in swords_IE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ec9e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FR_use['processed_text'] = df_FR_use['text'].str.lower()\\\n",
    "          .str.replace('(@[a-z0-9]+)\\w+',' ')\\\n",
    "          .str.replace('(http\\S+)', ' ')\\\n",
    "          .str.replace('/^[a-zA-ZÀ-Ÿ]*$/',' ')\\\n",
    "          .str.replace(' +',' ')\\\n",
    "          .str.replace('amp',' ')\\\n",
    "          .apply(lambda x: [i for i in x.split() if not i in swords_FR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bb19ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_UK_use['processed_text'] = df_UK_use['text'].str.lower()\\\n",
    "          .str.replace('(@[a-z0-9]+)\\w+',' ')\\\n",
    "          .str.replace('(http\\S+)', ' ')\\\n",
    "          .str.replace('([^0-9a-z \\t])',' ')\\\n",
    "          .str.replace(' +',' ')\\\n",
    "          .str.replace('amp',' ')\\\n",
    "          .apply(lambda x: [i for i in x.split() if not i in swords_UK])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2c0a81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d4307",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IE.dropna(inplace=True)\n",
    "df_IE.reset_index(drop=True, inplace=True)\n",
    "df_FR.dropna(inplace=True)\n",
    "df_FR.reset_index(drop=True, inplace=True)\n",
    "df_UK.dropna(inplace=True)\n",
    "df_UK.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891a0980",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "# #Ireland\n",
    "bigstring = df_IE_use['processed_text'].apply(lambda x: ' '.join(x)).str.cat(sep=' ')\n",
    "plt.figure(figsize=(12,12))\n",
    "wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                          background_color='white',\n",
    "                          collocations=False,\n",
    "                          width=1200,\n",
    "                          height=1000\n",
    "                         ).generate(bigstring)\n",
    "plt.axis('off')\n",
    "plt.title('Ireland Word Cloud', fontsize=20)\n",
    "plt.imshow(wordcloud)\n",
    "\n",
    "#France\n",
    "bigstring = df_FR_use['processed_text'].apply(lambda x: ' '.join(x)).str.cat(sep=' ')\n",
    "plt.figure(figsize=(12,12))\n",
    "wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                          background_color='white',\n",
    "                          collocations=False,\n",
    "                          width=1200,\n",
    "                          height=1000\n",
    "                         ).generate(bigstring)\n",
    "plt.axis('off')\n",
    "plt.title('France Word Cloud', fontsize=20)\n",
    "plt.imshow(wordcloud)\n",
    "\n",
    "#United Kingdom\n",
    "bigstring = df_UK_use['processed_text'].apply(lambda x: ' '.join(x)).str.cat(sep=' ')\n",
    "plt.figure(figsize=(12,12))\n",
    "wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                          background_color='white',\n",
    "                          collocations=False,\n",
    "                          width=1200,\n",
    "                          height=1000\n",
    "                         ).generate(bigstring)\n",
    "plt.axis('off')\n",
    "plt.title('Great Britain Word Cloud', fontsize=20)\n",
    "plt.imshow(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd19cc84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aea3076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "     return Series(dict(Number_of_tweets = x['tweet_id'].count(), ))\n",
    "\n",
    "daily_count = time_series_df.groupby(time_series_df.index.date).apply(f)\n",
    "print('There are', len(daily_count), 'days in the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d70205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
